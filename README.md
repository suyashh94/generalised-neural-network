# generalised-neural-network
This is an implementation of fully connected neural network from scratch. Batch normalization,  regularization using dropout , and momentum, SGD, and adam optimization techniques have been implemented.

You can find the implementation of a simple neural network and then progressively go on to see it's implementation with dropout, batch norm and other optimization techniques by going through individual files starting with nn_debug. 

nn_debug_batch_norm is a completely updated script that asks your input for every aspect of implementation. 

X - both train and test must be preprocessed and should be input as n X m , where n is the number of features and m is the training sample.

Y - both train and test must be of c X m where c is the number of classes and m is the number of training samples. 

